2023-02-12 05:14:21.551566: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-02-12 05:14:21.551648: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-02-12 05:14:21.551654: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
dataset construction done. 
Shape of X torch.Size([4937, 8]). 
Shape of Y torch.Size([4937])
493 labeled samples and 4444 unlabeled samples. Total: 4937
dataset construction done. 
Shape of X torch.Size([1235, 8]). 
Shape of Y torch.Size([1235])
247 labeled samples and 988 unlabeled samples. Total: 1235
Numer of layers 4
train net...
train with 4937 samples (with replacement) in one epoch
Epoch 0
  3.46% | time:   3.5s (  0.1m) | step:     10 | train acc: 0.641 | train loss: 0.654 |warm_t acc: 0.609 (0.608, 0.610) | warm_t eod gap: 0.1636
  3.46% | time:   1.3s (  0.1m) | step:     10 | train acc: 0.641 | train loss: 0.654 |warm_v acc: 0.607 (0.613, 0.602) | warm_v eod gap: 0.1208
  6.57% | time:   1.1s (  0.1m) | step:     19 | train acc: 0.703 | train loss: 0.601 |warm_t acc: 0.682 (0.659, 0.705) | warm_t eod gap: 0.2553
  6.57% | time:   0.2s (  0.1m) | step:     19 | train acc: 0.703 | train loss: 0.601 |warm_v acc: 0.619 (0.621, 0.618) | warm_v eod gap: 0.2598
Epoch 1
 10.03% | time:   1.2s (  0.1m) | step:     29 | train acc: 0.688 | train loss: 0.590 |warm_t acc: 0.670 (0.651, 0.689) | warm_t eod gap: 0.2627
 10.03% | time:   0.2s (  0.1m) | step:     29 | train acc: 0.688 | train loss: 0.590 |warm_v acc: 0.640 (0.621, 0.659) | warm_v eod gap: 0.2284
 13.15% | time:   1.1s (  0.1m) | step:     38 | train acc: 0.672 | train loss: 0.581 |warm_t acc: 0.674 (0.657, 0.691) | warm_t eod gap: 0.2697
 13.15% | time:   0.2s (  0.1m) | step:     38 | train acc: 0.672 | train loss: 0.581 |warm_v acc: 0.652 (0.621, 0.683) | warm_v eod gap: 0.2163
Epoch 2
 16.61% | time:   1.1s (  0.2m) | step:     48 | train acc: 0.734 | train loss: 0.567 |warm_t acc: 0.673 (0.655, 0.691) | warm_t eod gap: 0.2749
 16.61% | time:   0.2s (  0.2m) | step:     48 | train acc: 0.734 | train loss: 0.567 |warm_v acc: 0.648 (0.629, 0.667) | warm_v eod gap: 0.2096
 19.72% | time:   1.1s (  0.2m) | step:     57 | train acc: 0.703 | train loss: 0.560 |warm_t acc: 0.673 (0.653, 0.693) | warm_t eod gap: 0.2736
 19.72% | time:   0.2s (  0.2m) | step:     57 | train acc: 0.703 | train loss: 0.560 |warm_v acc: 0.644 (0.645, 0.642) | warm_v eod gap: 0.2033
Epoch 3
 23.18% | time:   1.1s (  0.2m) | step:     67 | train acc: 0.692 | train loss: 0.559 |warm_t acc: 0.668 (0.647, 0.689) | warm_t eod gap: 0.2679
 23.18% | time:   0.2s (  0.2m) | step:     67 | train acc: 0.692 | train loss: 0.559 |warm_v acc: 0.648 (0.645, 0.650) | warm_v eod gap: 0.1939
 26.30% | time:   1.1s (  0.2m) | step:     76 | train acc: 0.695 | train loss: 0.581 |warm_t acc: 0.666 (0.647, 0.685) | warm_t eod gap: 0.2736
 26.30% | time:   0.2s (  0.2m) | step:     76 | train acc: 0.695 | train loss: 0.581 |warm_v acc: 0.644 (0.645, 0.642) | warm_v eod gap: 0.1871
Epoch 4
 30.10% | time:   1.0s (  0.3m) | step:     87 | train acc: 0.696 | train loss: 0.580 |warm_t acc: 0.665 (0.643, 0.687) | warm_t eod gap: 0.2549
 30.10% | time:   0.2s (  0.3m) | step:     87 | train acc: 0.696 | train loss: 0.580 |warm_v acc: 0.644 (0.645, 0.642) | warm_v eod gap: 0.1871
 33.22% | time:   1.1s (  0.3m) | step:     96 | train acc: 0.715 | train loss: 0.567 |warm_t acc: 0.665 (0.643, 0.687) | warm_t eod gap: 0.2549
 33.22% | time:   0.2s (  0.3m) | step:     96 | train acc: 0.715 | train loss: 0.567 |warm_v acc: 0.644 (0.645, 0.642) | warm_v eod gap: 0.1871
Epoch 5
 36.68% | time:   1.1s (  0.3m) | step:    106 | train acc: 0.738 | train loss: 0.545 |test acc: 0.667 (0.647, 0.687) | test eod gap: 0.2634
 36.68% | time:   0.2s (  0.3m) | step:    106 | train acc: 0.738 | train loss: 0.545 |val acc: 0.648 (0.645, 0.650) | val eod gap: 0.1777
begin calculating influence
selected layers: [Traced<ShapedArray(float32[64])>with<DynamicJaxprTrace(level=0/1)>, Traced<ShapedArray(float32[8,64])>with<DynamicJaxprTrace(level=0/1)>, Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace(level=0/1)>, Traced<ShapedArray(float32[64,2])>with<DynamicJaxprTrace(level=0/1)>]
[Strategy 5] Acc of expected label: 0.670625
calculating influence -- done
dataset construction done. 
Shape of X torch.Size([4937, 8]). 
Shape of Y torch.Size([4937])
517 labeled samples and 4420 unlabeled samples. Total: 4937
Use 517 samples. Get 542 labels. Ratio: 0.9538745387453874
 39.79% | time:   3.0s (  0.3m) | step:    115 | train acc: 0.800 | train loss: 0.547 |test acc: 0.684 (0.651, 0.717) | test eod gap: 0.2726
 39.79% | time:   0.2s (  0.4m) | step:    115 | train acc: 0.800 | train loss: 0.547 |val acc: 0.652 (0.645, 0.659) | val eod gap: 0.2519
begin calculating influence
[Strategy 5] Acc of expected label: 0.6759375
calculating influence -- done
dataset construction done. 
Shape of X torch.Size([4937, 8]). 
Shape of Y torch.Size([4937])
549 labeled samples and 4388 unlabeled samples. Total: 4937
Use 549 samples. Get 612 labels. Ratio: 0.8970588235294118
Epoch 6
 43.25% | time:   2.4s (  0.4m) | step:    125 | train acc: 0.719 | train loss: 0.595 |test acc: 0.679 (0.645, 0.713) | test eod gap: 0.2597
 43.25% | time:   0.2s (  0.4m) | step:    125 | train acc: 0.719 | train loss: 0.595 |val acc: 0.656 (0.653, 0.659) | val eod gap: 0.2275
begin calculating influence
[Strategy 5] Acc of expected label: 0.6653125
calculating influence -- done
dataset construction done. 
Shape of X torch.Size([4937, 8]). 
Shape of Y torch.Size([4937])
581 labeled samples and 4356 unlabeled samples. Total: 4937
Use 581 samples. Get 683 labels. Ratio: 0.8506588579795022
 46.37% | time:   2.3s (  0.4m) | step:    134 | train acc: 0.715 | train loss: 0.593 |test acc: 0.682 (0.653, 0.711) | test eod gap: 0.2436
 46.37% | time:   0.2s (  0.4m) | step:    134 | train acc: 0.715 | train loss: 0.593 |val acc: 0.652 (0.661, 0.642) | val eod gap: 0.1710
begin calculating influence
[Strategy 5] Acc of expected label: 0.6640625
calculating influence -- done
dataset construction done. 
Shape of X torch.Size([4937, 8]). 
Shape of Y torch.Size([4937])
613 labeled samples and 4324 unlabeled samples. Total: 4937
Use 613 samples. Get 746 labels. Ratio: 0.82171581769437
Epoch 7
 49.83% | time:   2.3s (  0.5m) | step:    144 | train acc: 0.750 | train loss: 0.580 |test acc: 0.677 (0.655, 0.699) | test eod gap: 0.2281
 49.83% | time:   0.2s (  0.5m) | step:    144 | train acc: 0.750 | train loss: 0.580 |val acc: 0.632 (0.653, 0.610) | val eod gap: 0.1843
begin calculating influence
[Strategy 5] Acc of expected label: 0.6621875
calculating influence -- done
dataset construction done. 
Shape of X torch.Size([4937, 8]). 
Shape of Y torch.Size([4937])
632 labeled samples and 4305 unlabeled samples. Total: 4937
Use 632 samples. Get 777 labels. Ratio: 0.8133848133848134
 52.94% | time:   2.1s (  0.5m) | step:    153 | train acc: 0.809 | train loss: 0.515 |test acc: 0.681 (0.659, 0.703) | test eod gap: 0.2383
 52.94% | time:   0.2s (  0.5m) | step:    153 | train acc: 0.809 | train loss: 0.515 |val acc: 0.640 (0.653, 0.626) | val eod gap: 0.2163
begin calculating influence
[Strategy 5] Acc of expected label: 0.66
calculating influence -- done
dataset construction done. 
Shape of X torch.Size([4937, 8]). 
Shape of Y torch.Size([4937])
664 labeled samples and 4273 unlabeled samples. Total: 4937
Use 664 samples. Get 900 labels. Ratio: 0.7377777777777778
Epoch 8
 56.75% | time:   2.3s (  0.6m) | step:    164 | train acc: 0.777 | train loss: 0.547 |test acc: 0.677 (0.657, 0.697) | test eod gap: 0.2487
 56.75% | time:   0.2s (  0.6m) | step:    164 | train acc: 0.777 | train loss: 0.547 |val acc: 0.632 (0.645, 0.618) | val eod gap: 0.2174
begin calculating influence
[Strategy 5] Acc of expected label: 0.6459375
calculating influence -- done
dataset construction done. 
Shape of X torch.Size([4937, 8]). 
Shape of Y torch.Size([4937])
693 labeled samples and 4244 unlabeled samples. Total: 4937
Use 693 samples. Get 962 labels. Ratio: 0.7203742203742204
 59.86% | time:   2.2s (  0.6m) | step:    173 | train acc: 0.797 | train loss: 0.543 |test acc: 0.678 (0.663, 0.693) | test eod gap: 0.2288
 59.86% | time:   0.2s (  0.6m) | step:    173 | train acc: 0.797 | train loss: 0.543 |val acc: 0.619 (0.637, 0.602) | val eod gap: 0.1947
begin calculating influence
[Strategy 5] Acc of expected label: 0.6440625
calculating influence -- done
dataset construction done. 
Shape of X torch.Size([4937, 8]). 
Shape of Y torch.Size([4937])
710 labeled samples and 4227 unlabeled samples. Total: 4937
Use 710 samples. Get 998 labels. Ratio: 0.7114228456913828
Epoch 9
 63.32% | time:   2.4s (  0.6m) | step:    183 | train acc: 0.789 | train loss: 0.528 |test acc: 0.679 (0.665, 0.693) | test eod gap: 0.2322
 63.32% | time:   0.2s (  0.6m) | step:    183 | train acc: 0.789 | train loss: 0.528 |val acc: 0.628 (0.637, 0.618) | val eod gap: 0.2264
begin calculating influence
[Strategy 5] Acc of expected label: 0.643125
calculating influence -- done
dataset construction done. 
Shape of X torch.Size([4937, 8]). 
Shape of Y torch.Size([4937])
742 labeled samples and 4195 unlabeled samples. Total: 4937
Use 742 samples. Get 1055 labels. Ratio: 0.7033175355450237
 66.44% | time:   2.1s (  0.7m) | step:    192 | train acc: 0.812 | train loss: 0.500 |test acc: 0.675 (0.665, 0.685) | test eod gap: 0.2211
 66.44% | time:   0.2s (  0.7m) | step:    192 | train acc: 0.812 | train loss: 0.500 |val acc: 0.615 (0.637, 0.593) | val eod gap: 0.1883
begin calculating influence
[Strategy 5] Acc of expected label: 0.6415625
calculating influence -- done
dataset construction done. 
Shape of X torch.Size([4937, 8]). 
Shape of Y torch.Size([4937])
763 labeled samples and 4174 unlabeled samples. Total: 4937
Use 763 samples. Get 1092 labels. Ratio: 0.6987179487179487
Epoch 10
 69.90% | time:   2.3s (  0.7m) | step:    202 | train acc: 0.781 | train loss: 0.538 |test acc: 0.670 (0.657, 0.683) | test eod gap: 0.2148
 69.90% | time:   0.2s (  0.7m) | step:    202 | train acc: 0.781 | train loss: 0.538 |val acc: 0.607 (0.629, 0.585) | val eod gap: 0.1898
begin calculating influence
[Strategy 5] Acc of expected label: 0.6409375
calculating influence -- done
dataset construction done. 
Shape of X torch.Size([4937, 8]). 
Shape of Y torch.Size([4937])
770 labeled samples and 4167 unlabeled samples. Total: 4937
Use 770 samples. Get 1107 labels. Ratio: 0.6955736224028907
Traceback (most recent call last):
  File "run_compas.py", line 98, in <module>
    train_general(args)
  File "/home/zwzhu/code/fair-active-sampling/src/train.py", line 524, in train_general
    state, train_metric = train_step(state, batch)
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/_src/traceback_util.py", line 162, in reraise_with_filtered_traceback
    return fun(*args, **kwargs)
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/_src/api.py", line 622, in cache_miss
    execute = dispatch._xla_call_impl_lazy(fun_, *tracers, **params)
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/_src/dispatch.py", line 236, in _xla_call_impl_lazy
    return xla_callable(fun, device, backend, name, donated_invars, keep_unused,
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/linear_util.py", line 303, in memoized_fun
    ans = call(fun, *args)
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/_src/dispatch.py", line 359, in _xla_callable_uncached
    return lower_xla_callable(fun, device, backend, name, donated_invars, False,
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/_src/profiler.py", line 314, in wrapper
    return func(*args, **kwargs)
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/_src/dispatch.py", line 445, in lower_xla_callable
    jaxpr, out_type, consts = pe.trace_to_jaxpr_final2(
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/_src/profiler.py", line 314, in wrapper
    return func(*args, **kwargs)
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/interpreters/partial_eval.py", line 2077, in trace_to_jaxpr_final2
    jaxpr, out_type, consts = trace_to_subjaxpr_dynamic2(fun, main, debug_info)
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/interpreters/partial_eval.py", line 2027, in trace_to_subjaxpr_dynamic2
    ans = fun.call_wrapped(*in_tracers_)
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/linear_util.py", line 167, in call_wrapped
    ans = self.f(*args, **dict(self.params, **kwargs))
  File "/home/zwzhu/code/fair-active-sampling/src/train_state.py", line 118, in train_plain
    aux, grads = grad_fn(state.params)
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/_src/traceback_util.py", line 162, in reraise_with_filtered_traceback
    return fun(*args, **kwargs)
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/_src/api.py", line 1169, in value_and_grad_f
    ans, vjp_py, aux = _vjp(
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/_src/api.py", line 2661, in _vjp
    out_primal, out_vjp, aux = ad.vjp(
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/interpreters/ad.py", line 137, in vjp
    out_primals, pvals, jaxpr, consts, aux = linearize(traceable, *primals, has_aux=True)
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/interpreters/ad.py", line 124, in linearize
    jaxpr, out_pvals, consts = pe.trace_to_jaxpr_nounits(jvpfun_flat, in_pvals)
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/_src/profiler.py", line 314, in wrapper
    return func(*args, **kwargs)
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/interpreters/partial_eval.py", line 767, in trace_to_jaxpr_nounits
    jaxpr, (out_pvals, consts, env) = fun.call_wrapped(pvals)
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/linear_util.py", line 167, in call_wrapped
    ans = self.f(*args, **dict(self.params, **kwargs))
  File "/home/zwzhu/code/fair-active-sampling/src/loss_func.py", line 43, in loss_fn
    loss = cross_entropy_loss(logits=logits, labels=batch['label'])
  File "/home/zwzhu/code/fair-active-sampling/src/metrics.py", line 11, in cross_entropy_loss
    return cross_entropy_loss_per_sample(logits, labels).mean()
  File "/home/zwzhu/code/fair-active-sampling/src/metrics.py", line 17, in cross_entropy_loss_per_sample
    return optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=labels)
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/optax/_src/loss.py", line 205, in softmax_cross_entropy_with_integer_labels
    label_logits = jnp.take_along_axis(logits, labels[..., None], axis=-1)[..., 0]
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/_src/traceback_util.py", line 162, in reraise_with_filtered_traceback
    return fun(*args, **kwargs)
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/_src/api.py", line 626, in cache_miss
    top_trace.process_call(primitive, fun_, tracers, params))
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/interpreters/ad.py", line 339, in process_call
    result = call_primitive.bind(_update_annotation(f_jvp, f.in_type, which_nz),
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/core.py", line 2019, in bind
    outs = top_trace.process_call(self, fun_, tracers, params)
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/interpreters/partial_eval.py", line 265, in process_call
    out = primitive.bind(_update_annotation_known(f_, f.in_type, in_knowns),
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/core.py", line 2019, in bind
    outs = top_trace.process_call(self, fun_, tracers, params)
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/interpreters/partial_eval.py", line 1739, in process_call
    jaxpr, out_type, consts = trace_to_subjaxpr_dynamic2(f, self.main, debug_info=dbg)
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/interpreters/partial_eval.py", line 2027, in trace_to_subjaxpr_dynamic2
    ans = fun.call_wrapped(*in_tracers_)
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/linear_util.py", line 167, in call_wrapped
    ans = self.f(*args, **dict(self.params, **kwargs))
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py", line 3699, in take_along_axis
    raise ValueError(msg.format(ndim(indices), ndim(arr)))
jax._src.traceback_util.UnfilteredStackTrace: ValueError: indices and arr must have the same number of dimensions; 2 vs. 1

The stack trace below excludes JAX-internal frames.
The preceding is the original exception that occurred, unmodified.

--------------------

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "run_compas.py", line 98, in <module>
    train_general(args)
  File "/home/zwzhu/code/fair-active-sampling/src/train.py", line 524, in train_general
    state, train_metric = train_step(state, batch)
  File "/home/zwzhu/code/fair-active-sampling/src/train_state.py", line 118, in train_plain
    aux, grads = grad_fn(state.params)
  File "/home/zwzhu/code/fair-active-sampling/src/loss_func.py", line 43, in loss_fn
    loss = cross_entropy_loss(logits=logits, labels=batch['label'])
  File "/home/zwzhu/code/fair-active-sampling/src/metrics.py", line 11, in cross_entropy_loss
    return cross_entropy_loss_per_sample(logits, labels).mean()
  File "/home/zwzhu/code/fair-active-sampling/src/metrics.py", line 17, in cross_entropy_loss_per_sample
    return optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=labels)
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/optax/_src/loss.py", line 205, in softmax_cross_entropy_with_integer_labels
    label_logits = jnp.take_along_axis(logits, labels[..., None], axis=-1)[..., 0]
  File "/home/zwzhu/fair_active_sampling/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py", line 3699, in take_along_axis
    raise ValueError(msg.format(ndim(indices), ndim(arr)))
ValueError: indices and arr must have the same number of dimensions; 2 vs. 1
